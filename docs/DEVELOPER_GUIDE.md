# TaskFlow Developer Guide

> **Complete guide for developers contributing to TaskFlow**

This guide covers everything you need to set up, develop, test, and contribute to TaskFlow.

---

## Table of Contents

1. [Development Setup](#development-setup)
2. [Project Structure](#project-structure)
3. [Architecture Overview](#architecture-overview)
4. [Development Workflow](#development-workflow)
5. [Code Standards](#code-standards)
6. [Testing](#testing)
7. [Adding Features](#adding-features)
8. [Contributing](#contributing)

---

## Development Setup

### Prerequisites

**Required:**
- Python 3.10 or higher
- pip (Python package manager)
- Git

**Optional:**
- Docker & Docker Compose
- Virtual environment manager (venv, conda)

**Check Versions:**

```bash
python --version  # Should be 3.10+
pip --version
git --version
```

---

### Local Setup

**1. Clone Repository**

```bash
git clone https://github.com/yourusername/taskflow-sentiment-analysis.git
cd taskflow-sentiment-analysis
```

**2. Create Virtual Environment**

```bash
# Using venv
python -m venv .venv

# Activate (Mac/Linux)
source .venv/bin/activate

# Activate (Windows)
.venv\Scripts\activate
```

**3. Install Dependencies**

```bash
# Production dependencies
pip install -r requirements.txt

# Development dependencies (includes testing, linting)
pip install -r requirements-dev.txt
```

**4. Configure Environment**

```bash
# Copy example env file
cp .env.example .env

# Edit .env file
nano .env
```

**Required Environment Variables:**

```bash
# .env
ANTHROPIC_API_KEY=sk-ant-your-key-here
OPENAI_API_KEY=sk-your-key-here  # Optional, for GPT-4

# Optional
HF_TOKEN=hf_your_token_here  # For private HuggingFace models
LOG_LEVEL=INFO
MAX_WORKERS=4
```

**5. Verify Setup**

```bash
# Run tests
pytest tests/

# Start server
uvicorn src.api.main:app --reload --port 8000

# Visit http://localhost:8000
```

---

### Development Tools

**Code Quality:**

```bash
# Install pre-commit hooks
pip install pre-commit
pre-commit install

# Linter
pip install ruff

# Formatter
pip install black

# Type checker
pip install mypy
```

**IDE Setup:**

**VS Code (Recommended):**

```json
// .vscode/settings.json
{
  "python.linting.enabled": true,
  "python.linting.ruffEnabled": true,
  "python.formatting.provider": "black",
  "python.formatting.blackArgs": ["--line-length", "100"],
  "editor.formatOnSave": true,
  "python.testing.pytestEnabled": true
}
```

**PyCharm:**
- Enable Black formatter: Preferences ‚Üí Tools ‚Üí Black
- Enable pytest: Preferences ‚Üí Tools ‚Üí Python Integrated Tools ‚Üí Testing ‚Üí pytest

---

## Project Structure

```
taskflow-sentiment-analysis/
‚îú‚îÄ‚îÄ src/                          # Source code
‚îÇ   ‚îú‚îÄ‚îÄ agents/                   # 7 AI agents
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py              # BaseAgent class
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ column_detector.py  # Auto-detect text column
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_validator.py   # Clean & validate data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ orchestrator.py     # Coordinate tools
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pre_evaluator.py    # Quality check (pre-LLM)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ report_planner.py   # Plan report structure (LLM)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ report_writer.py    # Generate report text (LLM)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ report_evaluator.py # Validate report quality (LLM)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ report_generator.py # Orchestrate report generation
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ tools/                    # 5 ML analysis tools
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py              # BaseTool class
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sentiment_tool.py   # Sentiment analysis
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ emotion_tool.py     # Emotion detection
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ topic_tool.py       # Topic extraction
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ entity_tool.py      # Named entity recognition
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ keyphrase_tool.py   # Keyphrase extraction
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ llm/                      # LLM integration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py              # BaseLLM interface
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ anthropic_llm.py    # Claude integration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai_llm.py       # GPT-4 integration (optional)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ mock_llm.py         # Mock for testing
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ api/                      # FastAPI server
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py              # App initialization
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ routes.py            # API endpoints
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py            # Pydantic request/response models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ storage.py           # In-memory job storage
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ jobs.py              # Background job processor
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ export/                   # Report export
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pdf_generator.py    # Markdown ‚Üí PDF conversion
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ models/                   # Data models
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schemas.py           # Pydantic schemas for agents
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ utils/                    # Utilities
‚îÇ       ‚îú‚îÄ‚îÄ logger.py            # Logging setup
‚îÇ       ‚îî‚îÄ‚îÄ exceptions.py        # Custom exceptions
‚îÇ
‚îú‚îÄ‚îÄ static/                       # Web UI
‚îÇ   ‚îú‚îÄ‚îÄ css/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ style.css            # Styling
‚îÇ   ‚îú‚îÄ‚îÄ js/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ app.js               # Frontend logic
‚îÇ   ‚îî‚îÄ‚îÄ index.html               # Main page
‚îÇ
‚îú‚îÄ‚îÄ tests/                        # Test suite
‚îÇ   ‚îú‚îÄ‚îÄ unit/                    # Unit tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_agents/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_tools/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_llm/
‚îÇ   ‚îú‚îÄ‚îÄ integration/             # Integration tests
‚îÇ   ‚îî‚îÄ‚îÄ fixtures/                # Test data & fixtures
‚îÇ
‚îú‚îÄ‚îÄ docs/                         # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ API_DOCUMENTATION.md
‚îÇ   ‚îú‚îÄ‚îÄ USER_GUIDE.md
‚îÇ   ‚îú‚îÄ‚îÄ DEVELOPER_GUIDE.md
‚îÇ   ‚îú‚îÄ‚îÄ DEPLOYMENT_GUIDE.md
‚îÇ   ‚îî‚îÄ‚îÄ TROUBLESHOOTING.md
‚îÇ
‚îú‚îÄ‚îÄ scripts/                      # Utility scripts
‚îÇ   ‚îú‚îÄ‚îÄ test_e2e.py              # End-to-end testing
‚îÇ   ‚îî‚îÄ‚îÄ test_real_api.py         # Real API testing
‚îÇ
‚îú‚îÄ‚îÄ configs/                      # Configuration files
‚îÇ   ‚îî‚îÄ‚îÄ models.yaml              # Model configurations
‚îÇ
‚îú‚îÄ‚îÄ .env.example                  # Environment template
‚îú‚îÄ‚îÄ .gitignore                   # Git ignore rules
‚îú‚îÄ‚îÄ requirements.txt             # Production dependencies
‚îú‚îÄ‚îÄ requirements-dev.txt         # Development dependencies
‚îú‚îÄ‚îÄ Dockerfile                   # Docker image
‚îú‚îÄ‚îÄ docker-compose.yml           # Docker Compose config
‚îú‚îÄ‚îÄ pytest.ini                   # Pytest configuration
‚îî‚îÄ‚îÄ README.md                    # Main documentation
```

---

## Architecture Overview

### Design Patterns

**1. Agent Pattern**

All agents inherit from `BaseAgent`:

```python
class BaseAgent(ABC):
    @abstractmethod
    def execute(self, input_data: BaseModel) -> BaseModel:
        """Execute agent logic."""
        pass
```

**Benefits:**
- Uniform interface
- Easy testing (mock agents)
- Clear contracts

**2. Strategy Pattern (LLM)**

```python
class BaseLLM(ABC):
    @abstractmethod
    def generate(self, prompt: str) -> Dict:
        pass

# Implementations: ClaudeLLM, OpenAILLM, MockLLM
```

**3. Factory Pattern (Tools)**

```python
class ToolFactory:
    @staticmethod
    def create_tool(tool_name: str, config: ToolConfig):
        return TOOLS[tool_name](config)
```

---

### Data Flow

```
CSV Upload
    ‚Üì
ColumnDetectorAgent ‚Üí Detect text column
    ‚Üì
DataValidatorAgent ‚Üí Clean & validate
    ‚Üì
AnalysisOrchestratorAgent ‚Üí Run 5 tools in parallel
    ‚îú‚îÄ SentimentTool
    ‚îú‚îÄ EmotionTool
    ‚îú‚îÄ TopicTool
    ‚îú‚îÄ EntityTool
    ‚îî‚îÄ KeyphraseTool
    ‚Üì
PrePromptEvaluatorAgent ‚Üí Quality check
    ‚Üì
ReportGenerator ‚Üí Orchestrate report generation
    ‚îú‚îÄ ReportPlannerAgent (LLM)
    ‚îú‚îÄ ReportWriterAgent (LLM)
    ‚îî‚îÄ ReportEvaluatorAgent (LLM)
    ‚Üì
PDF/Markdown Export
```

---

### Agent Lifecycle

```python
# 1. Initialize
agent = SomeAgent(config)

# 2. Validate input
input_data = SomeInput(**data)

# 3. Execute
output = agent.execute(input_data)

# 4. Log
agent.log_execution(input_data, output)

# 5. Return
return output
```

---

## Development Workflow

### Creating a New Branch

```bash
# Update main
git checkout main
git pull origin main

# Create feature branch
git checkout -b feature/your-feature-name
```

### Making Changes

```bash
# 1. Write code
# 2. Write tests
# 3. Run tests
pytest tests/

# 4. Format code
black src/

# 5. Check linting
ruff check src/

# 6. Type check (optional)
mypy src/
```

### Committing Changes

```bash
# Stage changes
git add .

# Commit with descriptive message
git commit -m "feat: Add new feature

- Implement feature X
- Add tests for feature X
- Update documentation"

# Push to remote
git push origin feature/your-feature-name
```

### Creating Pull Request

1. Push branch to GitHub
2. Open Pull Request
3. Describe changes
4. Link related issues
5. Request review
6. Address feedback
7. Merge when approved

---

## Code Standards

### Python Style Guide

**Follow PEP 8 with these specifics:**

- **Line Length:** 100 characters
- **Indentation:** 4 spaces
- **Quotes:** Double quotes for strings
- **Imports:** Organized (stdlib, third-party, local)

**Example:**

```python
"""Module docstring.

Detailed description of what this module does.
"""

from typing import Dict, List, Optional

from pydantic import BaseModel

from src.utils.logger import setup_logger


class MyClass:
    """Class docstring.
    
    Detailed description of what this class does.
    """
    
    def __init__(self, config: Dict):
        """Initialize MyClass.
        
        Args:
            config: Configuration dictionary
        """
        self.config = config
        self.logger = setup_logger(self.__class__.__name__)
    
    def process(self, data: List[str]) -> Optional[Dict]:
        """Process data.
        
        Args:
            data: List of strings to process
            
        Returns:
            Processed data dictionary or None if failed
            
        Raises:
            ValueError: If data is empty
        """
        if not data:
            raise ValueError("Data cannot be empty")
        
        result = self._internal_process(data)
        return result
```

---

### Docstrings

**Use Google style:**

```python
def function_name(param1: str, param2: int) -> bool:
    """Short description.
    
    Longer description if needed. Can span multiple lines
    and include details about the function's behavior.
    
    Args:
        param1: Description of param1
        param2: Description of param2
        
    Returns:
        Description of return value
        
    Raises:
        ValueError: When param2 is negative
        TypeError: When param1 is not a string
        
    Example:
        >>> function_name("test", 5)
        True
    """
    pass
```

---

### Type Hints

**Always use type hints:**

```python
# Good
def process_data(data: List[str], threshold: float = 0.5) -> Dict[str, int]:
    pass

# Bad
def process_data(data, threshold=0.5):
    pass
```

---

### Error Handling

```python
from src.utils.exceptions import TaskFlowError

class SomeAgent(BaseAgent):
    def execute(self, input_data: SomeInput) -> SomeOutput:
        try:
            # Validate
            if not self.validate_input(input_data):
                raise TaskFlowError("Invalid input")
            
            # Process
            result = self._process(input_data)
            
            # Log
            self.log_execution(input_data, result)
            
            return result
            
        except TaskFlowError:
            # Re-raise TaskFlow errors
            raise
        except Exception as e:
            # Wrap unexpected errors
            self.logger.error(f"Unexpected error: {e}")
            raise TaskFlowError(f"Agent execution failed: {e}") from e
```

---

### Naming Conventions

| Type | Convention | Example |
|------|------------|---------|
| **Module** | lowercase_underscore | `sentiment_tool.py` |
| **Class** | PascalCase | `SentimentTool` |
| **Function** | lowercase_underscore | `process_data()` |
| **Variable** | lowercase_underscore | `input_data` |
| **Constant** | UPPERCASE | `MAX_RETRIES` |
| **Private** | _leading_underscore | `_internal_method()` |

---

## Testing

### Test Structure

```
tests/
‚îú‚îÄ‚îÄ unit/                    # Unit tests (isolated)
‚îÇ   ‚îú‚îÄ‚îÄ test_agents/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_column_detector.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_data_validator.py
‚îÇ   ‚îú‚îÄ‚îÄ test_tools/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_sentiment_tool.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_emotion_tool.py
‚îÇ   ‚îî‚îÄ‚îÄ test_llm/
‚îÇ       ‚îî‚îÄ‚îÄ test_mock_llm.py
‚îÇ
‚îú‚îÄ‚îÄ integration/             # Integration tests
‚îÇ   ‚îú‚îÄ‚îÄ test_pipeline.py
‚îÇ   ‚îî‚îÄ‚îÄ test_report_generation.py
‚îÇ
‚îî‚îÄ‚îÄ fixtures/                # Test data
    ‚îú‚îÄ‚îÄ sample_data.csv
    ‚îî‚îÄ‚îÄ test_config.yaml
```

---

### Writing Unit Tests

```python
"""Unit tests for SentimentTool."""

import pytest

from src.tools.sentiment_tool import SentimentTool


class TestSentimentTool:
    """Test suite for SentimentTool."""
    
    @pytest.fixture
    def tool(self):
        """Create tool instance."""
        return SentimentTool(device="cpu")
    
    def test_analyze_positive(self, tool):
        """Test positive sentiment detection."""
        result = tool.analyze("This is great!")
        
        assert result["label"] == "positive"
        assert result["score"] > 0.5
    
    def test_analyze_negative(self, tool):
        """Test negative sentiment detection."""
        result = tool.analyze("This is terrible!")
        
        assert result["label"] == "negative"
        assert result["score"] > 0.5
    
    def test_analyze_batch(self, tool):
        """Test batch processing."""
        texts = ["Good", "Bad", "Okay"]
        results = tool.analyze_batch(texts)
        
        assert len(results) == 3
        assert all("label" in r for r in results)
    
    def test_empty_input(self, tool):
        """Test empty input handling."""
        with pytest.raises(ValueError):
            tool.analyze("")
```

---

### Running Tests

```bash
# Run all tests
pytest tests/

# Run specific test file
pytest tests/unit/test_sentiment_tool.py

# Run specific test
pytest tests/unit/test_sentiment_tool.py::TestSentimentTool::test_analyze_positive

# Run with coverage
pytest --cov=src tests/

# Run with verbose output
pytest -v tests/

# Run only failed tests
pytest --lf

# Run in parallel (faster)
pytest -n auto tests/
```

---

### Test Coverage

**Target:** >70% coverage

```bash
# Generate coverage report
pytest --cov=src --cov-report=html tests/

# View report
open htmlcov/index.html
```

---

### Mocking

```python
from unittest.mock import Mock, patch

def test_with_mock_llm():
    """Test with mocked LLM."""
    mock_llm = Mock()
    mock_llm.generate.return_value = {
        "content": "Test report",
        "tokens": 100
    }
    
    agent = ReportWriterAgent(llm=mock_llm)
    result = agent.execute(test_input)
    
    assert result.report_text == "Test report"
    mock_llm.generate.assert_called_once()
```

---

## Adding Features

### Adding a New Agent

**1. Create Agent Class**

```python
# src/agents/my_new_agent.py

from src.agents.base import BaseAgent
from src.models.schemas import MyInput, MyOutput

class MyNewAgent(BaseAgent):
    """New agent description."""
    
    def execute(self, input_data: MyInput) -> MyOutput:
        """Execute agent logic.
        
        Args:
            input_data: Input data
            
        Returns:
            Output data
        """
        # Implementation
        result = self._process(input_data)
        return MyOutput(**result)
```

**2. Define Schemas**

```python
# src/models/schemas.py

class MyInput(BaseModel):
    """Input schema for MyNewAgent."""
    field1: str
    field2: int

class MyOutput(BaseModel):
    """Output schema for MyNewAgent."""
    result: str
    score: float
```

**3. Write Tests**

```python
# tests/unit/test_my_new_agent.py

class TestMyNewAgent:
    def test_execute_success(self):
        agent = MyNewAgent()
        input_data = MyInput(field1="test", field2=5)
        result = agent.execute(input_data)
        assert isinstance(result, MyOutput)
```

**4. Update Documentation**

- Add to `docs/AGENT_INTERFACES.md`
- Update architecture diagram
- Add usage example

---

### Adding a New Tool

**1. Create Tool Class**

```python
# src/tools/my_new_tool.py

from src.tools.base import BaseTool
from transformers import pipeline

class MyNewTool(BaseTool):
    """New tool description."""
    
    def _load_model(self):
        """Load model."""
        self.pipeline = pipeline(
            "task-name",
            model=self.model_name,
            device=0 if self.device == "cuda" else -1
        )
    
    def analyze(self, text: str) -> Dict:
        """Analyze text.
        
        Args:
            text: Input text
            
        Returns:
            Analysis results
        """
        result = self.pipeline(text)
        return {
            "label": result[0]["label"],
            "score": result[0]["score"]
        }
```

**2. Add to Orchestrator**

```python
# src/agents/orchestrator.py

class AnalysisOrchestratorAgent:
    def __init__(self, config: ToolConfig, device: str = "cpu"):
        # ... existing tools ...
        self.my_new_tool = MyNewTool(
            model_name=config.my_new_model,
            device=device
        )
```

**3. Update Config**

```python
# src/models/schemas.py

class ToolConfig(BaseModel):
    # ... existing configs ...
    my_new_model: str = "default/model-name"
```

---

### Adding a New Endpoint

**1. Define Models**

```python
# src/api/models.py

class MyRequest(BaseModel):
    """Request model."""
    param1: str
    param2: int

class MyResponse(BaseModel):
    """Response model."""
    result: str
    status: str
```

**2. Add Route**

```python
# src/api/routes.py

@router.post("/my-endpoint", response_model=MyResponse)
async def my_endpoint(request: MyRequest):
    """New endpoint description.
    
    Args:
        request: Request data
        
    Returns:
        Response data
    """
    # Implementation
    result = process_request(request)
    return MyResponse(result=result, status="success")
```

**3. Update API Docs**

- Add to `docs/API_DOCUMENTATION.md`
- Include example request/response
- Document error cases

---

## Contributing

### Contribution Workflow

1. **Fork** repository
2. **Clone** your fork
3. **Create** feature branch
4. **Make** changes
5. **Test** thoroughly
6. **Commit** with clear messages
7. **Push** to your fork
8. **Create** Pull Request

---

### Pull Request Guidelines

**Title:**
```
feat: Add sentiment analysis caching
fix: Correct column detection for edge case
docs: Update API documentation
test: Add integration tests for pipeline
```

**Description Template:**

```markdown
## Description
Brief description of changes

## Type of Change
- [ ] Bug fix
- [ ] New feature
- [ ] Breaking change
- [ ] Documentation update

## Testing
- [ ] Unit tests added/updated
- [ ] Integration tests pass
- [ ] Manual testing completed

## Checklist
- [ ] Code follows style guidelines
- [ ] Self-review completed
- [ ] Documentation updated
- [ ] No new warnings
- [ ] Tests pass locally
```

---

### Code Review Process

**As Author:**
1. Ensure all tests pass
2. Address linting issues
3. Update documentation
4. Respond to feedback promptly
5. Make requested changes

**As Reviewer:**
1. Check code quality
2. Verify tests exist
3. Test functionality
4. Review documentation
5. Provide constructive feedback

---

### Commit Message Convention

**Format:**
```
<type>(<scope>): <subject>

<body>

<footer>
```

**Types:**
- `feat`: New feature
- `fix`: Bug fix
- `docs`: Documentation
- `style`: Formatting
- `refactor`: Code restructuring
- `test`: Tests
- `chore`: Maintenance

**Example:**

```
feat(tools): Add caching for sentiment analysis

- Implement LRU cache for sentiment results
- Add cache configuration options
- Update tests to cover caching behavior

Closes #123
```

---

## Development Tips

### Debugging

**1. Use Logger**

```python
self.logger.debug(f"Processing {len(data)} items")
self.logger.info("Agent execution started")
self.logger.warning("Low confidence score detected")
self.logger.error("Failed to process data", exc_info=True)
```

**2. Interactive Debugging**

```python
# Add breakpoint
import pdb; pdb.set_trace()

# Or use built-in
breakpoint()
```

**3. Print Debugging**

```python
from pprint import pprint
pprint(complex_data_structure)
```

---

### Performance Profiling

```python
import cProfile
import pstats

# Profile code
profiler = cProfile.Profile()
profiler.enable()

# Your code here

profiler.disable()
stats = pstats.Stats(profiler)
stats.sort_stats('cumulative')
stats.print_stats(10)
```

---

### Hot Reload

```bash
# Server auto-reloads on file changes
uvicorn src.api.main:app --reload --port 8000
```

---

## Resources

**Documentation:**
- [FastAPI Docs](https://fastapi.tiangolo.com/)
- [Pydantic Docs](https://docs.pydantic.dev/)
- [HuggingFace Transformers](https://huggingface.co/docs/transformers)
- [Pytest Docs](https://docs.pytest.org/)

**Style Guides:**
- [PEP 8](https://pep8.org/)
- [Google Python Style](https://google.github.io/styleguide/pyguide.html)

**Tools:**
- [Black Formatter](https://black.readthedocs.io/)
- [Ruff Linter](https://docs.astral.sh/ruff/)
- [MyPy Type Checker](https://mypy.readthedocs.io/)

---

## Getting Help

**Questions?**

üí¨ **Discussions:** [GitHub Discussions](https://github.com/yourusername/taskflow/discussions)  
üêõ **Issues:** [GitHub Issues](https://github.com/yourusername/taskflow/issues)  
üìß **Email:** dev@taskflow.example.com

---

**Happy coding! üöÄ**